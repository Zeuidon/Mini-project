{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARCvQqmJkGPM",
        "outputId": "c31c3471-a4c6-4ffa-8ad7-a8df7f519be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: constants in /home/arvind/anaconda3/lib/python3.11/site-packages (0.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1Jg7TlveX2T",
        "outputId": "d33bd735-f719-45d4-e9ac-bfd59b06c0ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyPDF2<3.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (2.12.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install 'PyPDF2<3.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3N2E8XuOg2bt",
        "outputId": "cfba53c0-b3bf-4aad-9d54-2d7643052875"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/arvind/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
            "Requirement already satisfied: requests in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/arvind/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /home/arvind/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arvind/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arvind/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/arvind/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arvind/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/arvind/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hksrOvrgsjxz",
        "outputId": "7b2f783f-119a-4bde-97b0-62e12b10d1c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/arvind/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/arvind/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/arvind/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pickle\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import constants as const\n",
        "import PyPDF2\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "nltk.download(['punkt', 'stopwords', 'wordnet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdP9zp5eZnEL",
        "outputId": "314f1941-92bb-4604-f438-e3351e387ddb"
      },
      "outputs": [],
      "source": [
        "def tfidf_similarity(tokenized_text1, tokenized_text2):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform([tokenized_text1, tokenized_text2])\n",
        "\n",
        "    # Similarity Measurement\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])\n",
        "\n",
        "    return cosine_sim[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iUI3NxpzgjRa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R8tiWi0slVmA"
      },
      "outputs": [],
      "source": [
        "def compute_bert_embeddings(sentence, model, tokenizer):\n",
        "    input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_ids)\n",
        "    return output.last_hidden_state.mean(dim=1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2CnzW6i-lWQQ"
      },
      "outputs": [],
      "source": [
        "def combine_embeddings(long_text):\n",
        "  max_segment_length = 512  # Maximum segment length allowed by BERT\n",
        "  segments = [long_text[i:i + max_segment_length] for i in range(0, len(long_text), max_segment_length)]\n",
        "\n",
        "  # Initialize an empty tensor for aggregated embeddings\n",
        "  aggregated_embeddings = torch.zeros((1, 768))  # Ensure the size matches BERT's embedding size (e.g., 768)\n",
        "\n",
        "  # Process each segment separately and aggregate embeddings\n",
        "  for segment in segments:\n",
        "      encoded_input = tokenizer(segment, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "      with torch.no_grad():\n",
        "          output = model(**encoded_input)\n",
        "      segment_embedding = output.last_hidden_state.mean(dim=1)  # Mean pooling over tokens\n",
        "      aggregated_embeddings += segment_embedding\n",
        "  return aggregated_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnv-KSQ5lYDa",
        "outputId": "78ef6d65-1f0a-4651-f21b-23692e6510e9"
      },
      "outputs": [],
      "source": [
        "def bert_similarity(tokenized_text1, tokenized_text2) :\n",
        "    embeddings1 = combine_embeddings(tokenized_text1)\n",
        "    embeddings2 = combine_embeddings(tokenized_text2)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    return cosine_similarity(embeddings1, embeddings2)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /home/arvind/anaconda3/lib/python3.11/site-packages (0.97.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4 in /home/arvind/anaconda3/lib/python3.11/site-packages (from fastapi) (1.10.12)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<2.0.0,>=1.7.4->fastapi) (4.7.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /home/arvind/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/arvind/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydantic in /home/arvind/anaconda3/lib/python3.11/site-packages (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arvind/anaconda3/lib/python3.11/site-packages (from pydantic) (4.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [21645]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     127.0.0.1:59416 - \"POST /in HTTP/1.1\" 200 OK\n",
            "INFO:     127.0.0.1:42658 - \"POST /in HTTP/1.1\" 200 OK\n",
            "INFO:     127.0.0.1:37590 - \"POST /in HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [21645]\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Item(BaseModel):\n",
        "    t1: str\n",
        "    t2: str\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/in\")\n",
        "def create(item: Item):\n",
        "    dt = item.dict()\n",
        "    text1 = dt['t1']\n",
        "    text2 = dt['t2']\n",
        "    b = str(bert_similarity(text1, text2))\n",
        "    t =  str(tfidf_similarity(text1, text2))\n",
        "    return {\n",
        "        \"Bert similarity\": b, \n",
        "        \"Tfidf similarity\": t\n",
        "    }\n",
        "\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    uvicorn.run(app)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
